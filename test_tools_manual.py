"""
Script de prueba manual para el sistema de Tools.

Este script permite probar el sistema de Tools sin necesidad de
conectarse a Telegram, √∫til para desarrollo y debugging.

Uso:
    python test_tools_manual.py
"""
import asyncio
import logging
from src.config.settings import settings
from src.database.connection import DatabaseManager
from src.agent.llm_agent import LLMAgent
from src.tools import (
    initialize_builtin_tools,
    get_registry,
    get_tool_summary,
    ToolOrchestrator,
    ExecutionContextBuilder
)

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


async def test_tool_initialization():
    """Test 1: Verificar inicializaci√≥n de tools."""
    print("\n" + "="*60)
    print("TEST 1: Inicializaci√≥n de Tools")
    print("="*60)

    # Inicializar tools
    initialize_builtin_tools()

    # Obtener resumen
    summary = get_tool_summary()

    print(f"\n‚úÖ Tools registrados: {summary['total_tools']}")
    print(f"‚úÖ Comandos disponibles: {', '.join(summary['commands'])}")

    print("\nDetalles de tools:")
    for tool_info in summary['tools']:
        print(f"\n  üì¶ {tool_info['name']} (v{tool_info['version']})")
        print(f"     Descripci√≥n: {tool_info['description']}")
        print(f"     Comandos: {', '.join(tool_info['commands'])}")
        print(f"     Categor√≠a: {tool_info['category']}")
        print(f"     Requiere auth: {tool_info['requires_auth']}")

    return summary['total_tools'] > 0


async def test_query_tool_direct():
    """Test 2: Probar QueryTool directamente."""
    print("\n" + "="*60)
    print("TEST 2: QueryTool - Ejecuci√≥n Directa")
    print("="*60)

    try:
        # Inicializar componentes
        db_manager = DatabaseManager()
        llm_agent = LLMAgent(db_manager=db_manager)

        # Obtener QueryTool del registry
        registry = get_registry()
        query_tool = registry.get_tool_by_name("query")

        if not query_tool:
            print("‚ùå QueryTool no encontrado en el registry")
            return False

        # Crear contexto de ejecuci√≥n
        context = (
            ExecutionContextBuilder()
            .with_llm_agent(llm_agent)
            .with_db_manager(db_manager)
            .build()
        )

        # Ejecutar query de prueba
        test_query = "¬øCu√°ntos usuarios hay registrados en el sistema?"
        print(f"\nüîç Ejecutando query: {test_query}")

        result = await query_tool.execute(
            user_id=999,  # Usuario de prueba
            params={"query": test_query},
            context=context
        )

        # Mostrar resultado
        if result.success:
            print(f"\n‚úÖ Query ejecutada exitosamente")
            print(f"‚è±Ô∏è  Tiempo de ejecuci√≥n: {result.execution_time_ms:.2f}ms")
            print(f"\nüìä Respuesta:")
            print("-" * 60)
            print(result.data)
            print("-" * 60)
            return True
        else:
            print(f"\n‚ùå Error en query: {result.error}")
            print(f"   Mensaje usuario: {result.user_friendly_error}")
            return False

    except Exception as e:
        print(f"\n‚ùå Excepci√≥n durante test: {e}")
        logger.error("Error en test_query_tool_direct", exc_info=True)
        return False


async def test_orchestrator_flow():
    """Test 3: Probar flujo completo con ToolOrchestrator."""
    print("\n" + "="*60)
    print("TEST 3: ToolOrchestrator - Flujo Completo")
    print("="*60)

    try:
        # Inicializar componentes
        db_manager = DatabaseManager()
        llm_agent = LLMAgent(db_manager=db_manager)

        # Crear orquestador
        registry = get_registry()
        orchestrator = ToolOrchestrator(registry)

        # Crear contexto
        context = (
            ExecutionContextBuilder()
            .with_llm_agent(llm_agent)
            .with_db_manager(db_manager)
            .build()
        )

        # Ejecutar varias queries de prueba
        test_queries = [
            "¬øCu√°ntos usuarios hay?",
            "¬øQu√© tablas tiene la base de datos?",
            "Dame un resumen del sistema"
        ]

        success_count = 0
        for i, query in enumerate(test_queries, 1):
            print(f"\nüìù Query {i}/{len(test_queries)}: {query}")

            result = await orchestrator.execute_command(
                user_id=999,
                command="/ia",
                params={"query": query},
                context=context
            )

            if result.success:
                print(f"   ‚úÖ Exitosa ({result.execution_time_ms:.2f}ms)")
                success_count += 1
            else:
                print(f"   ‚ùå Fallida: {result.error}")

        # Mostrar estad√≠sticas del orquestador
        print("\nüìä Estad√≠sticas del Orquestador:")
        stats = orchestrator.get_stats()
        print(f"   Total ejecuciones: {stats['total_executions']}")
        print(f"   Total errores: {stats['total_errors']}")
        print(f"   Tasa de √©xito: {stats['success_rate']:.1f}%")

        return success_count == len(test_queries)

    except Exception as e:
        print(f"\n‚ùå Excepci√≥n durante test: {e}")
        logger.error("Error en test_orchestrator_flow", exc_info=True)
        return False


async def test_parameter_validation():
    """Test 4: Probar validaci√≥n de par√°metros."""
    print("\n" + "="*60)
    print("TEST 4: Validaci√≥n de Par√°metros")
    print("="*60)

    try:
        db_manager = DatabaseManager()
        llm_agent = LLMAgent(db_manager=db_manager)

        registry = get_registry()
        orchestrator = ToolOrchestrator(registry)

        context = (
            ExecutionContextBuilder()
            .with_llm_agent(llm_agent)
            .with_db_manager(db_manager)
            .build()
        )

        # Test con par√°metros inv√°lidos
        test_cases = [
            ("Query muy corta", "ab", False),
            ("Query v√°lida", "¬øCu√°ntos usuarios hay?", True),
            ("Query muy larga", "a" * 1001, False),
        ]

        success_count = 0
        for name, query, should_succeed in test_cases:
            print(f"\nüß™ {name}: '{query[:50]}...' (esperado: {'‚úÖ' if should_succeed else '‚ùå'})")

            result = await orchestrator.execute_command(
                user_id=999,
                command="/ia",
                params={"query": query},
                context=context
            )

            if result.success == should_succeed:
                print(f"   ‚úÖ Comportamiento correcto")
                success_count += 1
            else:
                print(f"   ‚ùå Comportamiento inesperado")
                if not result.success:
                    print(f"      Error: {result.user_friendly_error}")

        return success_count == len(test_cases)

    except Exception as e:
        print(f"\n‚ùå Excepci√≥n durante test: {e}")
        logger.error("Error en test_parameter_validation", exc_info=True)
        return False


async def test_error_handling():
    """Test 5: Probar manejo de errores."""
    print("\n" + "="*60)
    print("TEST 5: Manejo de Errores")
    print("="*60)

    try:
        # Test 1: Context sin LLMAgent
        print("\nüß™ Test: Context sin LLMAgent")
        registry = get_registry()
        orchestrator = ToolOrchestrator(registry)

        context_without_llm = ExecutionContextBuilder().build()

        result = await orchestrator.execute_command(
            user_id=999,
            command="/ia",
            params={"query": "test"},
            context=context_without_llm
        )

        if not result.success and "no disponible" in result.user_friendly_error.lower():
            print("   ‚úÖ Error manejado correctamente")
            test1_pass = True
        else:
            print("   ‚ùå Error no manejado correctamente")
            test1_pass = False

        # Test 2: Comando inexistente
        print("\nüß™ Test: Comando inexistente")
        db_manager = DatabaseManager()
        llm_agent = LLMAgent(db_manager=db_manager)

        context = (
            ExecutionContextBuilder()
            .with_llm_agent(llm_agent)
            .build()
        )

        result = await orchestrator.execute_command(
            user_id=999,
            command="/comando_que_no_existe",
            params={},
            context=context
        )

        if not result.success and "no encontrado" in result.error.lower():
            print("   ‚úÖ Error manejado correctamente")
            test2_pass = True
        else:
            print("   ‚ùå Error no manejado correctamente")
            test2_pass = False

        return test1_pass and test2_pass

    except Exception as e:
        print(f"\n‚ùå Excepci√≥n durante test: {e}")
        logger.error("Error en test_error_handling", exc_info=True)
        return False


async def run_all_tests():
    """Ejecutar todos los tests."""
    print("\n" + "="*60)
    print("üß™ SUITE DE PRUEBAS MANUALES - SISTEMA DE TOOLS")
    print("="*60)
    print(f"Provider LLM: {settings.openai_model if settings.openai_api_key else 'Anthropic'}")
    print(f"Base de datos: {settings.database_url[:30]}...")
    print("="*60)

    results = []

    # Ejecutar tests en orden
    tests = [
        ("Inicializaci√≥n de Tools", test_tool_initialization),
        ("QueryTool Directo", test_query_tool_direct),
        ("Flujo con Orquestador", test_orchestrator_flow),
        ("Validaci√≥n de Par√°metros", test_parameter_validation),
        ("Manejo de Errores", test_error_handling)
    ]

    for name, test_func in tests:
        try:
            result = await test_func()
            results.append((name, result))
        except Exception as e:
            logger.error(f"Error ejecutando {name}: {e}", exc_info=True)
            results.append((name, False))

    # Mostrar resumen
    print("\n" + "="*60)
    print("üìä RESUMEN DE PRUEBAS")
    print("="*60)

    passed = sum(1 for _, result in results if result)
    total = len(results)

    for name, result in results:
        status = "‚úÖ PAS√ì" if result else "‚ùå FALL√ì"
        print(f"{status} - {name}")

    print("="*60)
    print(f"\nResultado final: {passed}/{total} tests pasaron")
    print(f"Tasa de √©xito: {passed/total*100:.1f}%")

    if passed == total:
        print("\nüéâ ¬°Todos los tests pasaron exitosamente!")
        return 0
    else:
        print(f"\n‚ö†Ô∏è  {total - passed} test(s) fallaron")
        return 1


if __name__ == "__main__":
    print("\nüöÄ Iniciando pruebas del sistema de Tools...")
    print("‚ö†Ô∏è  Aseg√∫rate de tener configuradas las variables de entorno (.env)")
    print("‚ö†Ô∏è  Se realizar√°n consultas reales al LLM y a la base de datos\n")

    input("Presiona ENTER para continuar o Ctrl+C para cancelar...")

    exit_code = asyncio.run(run_all_tests())
    exit(exit_code)
